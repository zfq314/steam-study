

###### 1、flume taildirsourse文件重复

配置文件

flume的组件

source-channel-sink

flume的启动命令

/data/program/flume/bin/flume-ng agent --conf-file /data/program/flume/conf/test.conf --name a1  -Dflume.root.logger=INFO,console

taildirsource

​	优点：支持监控多目录，断点续传

​	缺点：如果监控的文件修改名字，就会重复消费，查看 taidir_position 的内容，inode+filename来做监控文件的判断

​				解决方案：

​				使用不改名字的日志框架 logback等

​				或者修改源码解决

如果把监控的的文件名字写死，会出现零点漂移，数据丢失的情况，接近零点的时候，任务失败，然后恢复。会造成数据丢失的情况。

test.conf，修改源码，监控文件如果重命名，数据不重复读

```
##组件
a1.sources = r1
a1.channels = c1
a1.sinks = k1

##source
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /data/program/flume/taildir_position.json
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /data/program/flume/files/file.*

##sink
a1.sinks.k1.type = logger
##channel
a1.channels.c1.type = memory

##拼装
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1


```

###### 2、修改源码的步骤：

1. ​	将原来的项目拷贝过来，远程拉取下来，

2. ​	然后将某个module放入到自己的项目中，修改pom文件

3. ​	修改代码，然后打包，修改源码的过程中，尽量不要修改包名和类名

4. ​	打包之后，然后上传到相应的服务器上面。然后到相应的lib上面，

5. ​	上传之后，先验证jar的功能是否使用，如果基本功能无异常，然后，测试自己的修改的模块，看是否能够完成自己的需求。


###### 3、提高代码质量

- 整理思路-看着敲--根据注释敲--自己敲

- 根据注释，自己敲

- 自己敲


###### 4、在数据采集的阶段 kafka 充当的角色，生产者和消费者

kafkachannel的好好处，省略了sink阶段

```
采集工程中的三种用法，也可做生产者也可以消费者，具体的实现
source-kafkanchannel-sink

source-kafkachannel

kafkachannel-sink
```

###### 5、sqoop导出数据

```
全量数据 ：where 1=1

增量数据：where create_time=当天

变化和新增： where create_time=当天 or operate_time=当天

特殊表：全量导出 
```



###### 6、面试过程的框架梳理

```
逻辑线：数据流-(某个框架)监控-优化-配置
大数据组件的复习方式，先学习用，在研究源码级别
```

###### 7、批量替换一个目录下所有文件的某个字符传

```
sed -i 's/hadoop31/hadoop33/g' ./*.json
```

###### 8、某个目录下文件的个数

```
ls -l | grep "^-" | wc -l
```

###### 9、kafka

​	producer 

​			ack 0 1 -1 

​      保证生产者丢失数据，设置ack -1 

​	 拦截器 序列化器 分区器 发送流程 sender main 幂等性，事务 

​	分区规则

​	有指定分区，则发指定分区

​	没有指定分区 ，则根据key值hash 

​	没有指定分区也没有key,轮询 

broker

​	topic

​		副本： 高可靠 isr leo hw

​		分区： 高并发，负载均衡，防止热点

consumer:

​	分区分配规则

​	offset保存

​	默认 _ _consumer_offsets 主题

​	其他 手动维护 offset （mysql）

```
保存数据&保存offset写到一个事务

精准一次消费
```

先保存数据后保存offset 重复数据+幂等性（精准一次消费）：（如果不能做到精准消费）

先保存offset后保存数据 丢失数据

###### 10、安装Homebrew老是443

安装Homebrew老是443的朋友可以试试国内镜像，几分钟搞定：
/bin/zsh -c "$(curl -fsSL https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)"

###### 11、Kj数据库导出字符串拼接 

```
SELECT CONCAT('import_',name,'()','{','\n',
'\t','import_data ',name,' ','"','select * from $database.',name,' where 1=1','"'
'\n','}') FROM `sheet1`


select CONCAT('"',name,'"',')','\n','\t','import_',name,'\n','\t',';;') from sheet1

数据装载字符串拼接
select CONCAT('load data inpath ',"'",'/kjdata/',name,'/$do_date', "'",' into table $db.',name, ' partition (dt=',"'",'$do_date',"'",');') from sheet1
```

###### 12、kj sqlserver获取行数 

```
SELECT a.name,b.rows FROM sysobjects a
INNER JOIN sysindexes b ON a.id=b.id
WHERE b.indid IN(0,1) AND a.Type='u' and b.rows<>0 and b.rows <10000
ORDER BY b.rows desc
```

```
SELECT a.name,b.rows FROM sysobjects a
INNER JOIN sysindexes b ON a.id=b.id
WHERE b.indid IN(0,1) AND a.Type='u' and b.rows<>0 and b.rows <10000
ORDER BY b.rows desc

SELECT a.name FROM sysobjects a
INNER JOIN sysindexes b ON a.id=b.id
WHERE b.indid IN(0,1) AND a.Type='u' and b.rows<>0 and b.rows <10000
ORDER BY b.rows asc

SELECT a.name,b.rows FROM sysobjects a
INNER JOIN sysindexes b ON a.id=b.id
WHERE b.indid IN(0,1) AND a.Type='u' and b.rows<>0 and b.rows >=10000 and b.rows <20000
ORDER BY b.rows desc 


SELECT a.name,b.rows FROM sysobjects a
INNER JOIN sysindexes b ON a.id=b.id
WHERE b.indid IN(0,1) AND a.Type='u' and b.rows<>0 and b.rows >=20000 and b.rows <30000
ORDER BY b.rows desc 




SELECT a.name,b.rows FROM sysobjects a
INNER JOIN sysindexes b ON a.id=b.id
WHERE b.indid IN(0,1) AND a.Type='u' and b.rows<>0 and b.rows >=30000 and b.rows <40000
ORDER BY b.rows desc 

SELECT a.name,b.rows FROM sysobjects a
INNER JOIN sysindexes b ON a.id=b.id
WHERE b.indid IN(0,1) AND a.Type='u' and b.rows<>0 and b.rows >=40000 and b.rows <100000
ORDER BY b.rows desc 


SELECT a.name,b.rows FROM sysobjects a
INNER JOIN sysindexes b ON a.id=b.id
WHERE b.indid IN(0,1) AND a.Type='u' and b.rows<>0 and b.rows >=100000 and b.rows <1000000
ORDER BY b.rows desc 



SELECT a.name,b.rows FROM sysobjects a
INNER JOIN sysindexes b ON a.id=b.id
WHERE b.indid IN(0,1) AND a.Type='u' and b.rows<>0 and b.rows >=1000000 
ORDER BY b.rows desc 

SELECT a.name,b.rows FROM sysobjects a
INNER JOIN sysindexes b ON a.id=b.id
WHERE b.indid IN(0,1) AND a.Type='u' and b.rows<>0 and b.rows >=1000000  and b.rows <80000000
ORDER BY b.rows asc  
```

```
SELECT CONCAT('import_',name,'()','{','\n',
'\t','import_data ',name,' ','"','select * from $database.',name,' where 1=1','"'
'\n','}') FROM `seven`;

select CONCAT('"',name,'"',')','\n','\t','import_',name,'\n','\t',';;') from seven;


select CONCAT('import_',name) from seven 

select CONCAT('load data inpath ',"'",'/kjdata/',name,'/$do_date', "'",' into table $db.',name, ' partition (dt=',"'",'$do_date',"'",');') from two union all  
select CONCAT('load data inpath ',"'",'/kjdata/',name,'/$do_date', "'",' into table $db.',name, ' partition (dt=',"'",'$do_date',"'",');') from three union all
select CONCAT('load data inpath ',"'",'/kjdata/',name,'/$do_date', "'",' into table $db.',name, ' partition (dt=',"'",'$do_date',"'",');') from four union  all
select CONCAT('load data inpath ',"'",'/kjdata/',name,'/$do_date', "'",' into table $db.',name, ' partition (dt=',"'",'$do_date',"'",');') from five union all
select CONCAT('load data inpath ',"'",'/kjdata/',name,'/$do_date', "'",' into table $db.',name, ' partition (dt=',"'",'$do_date',"'",');') from six union all
select CONCAT('load data inpath ',"'",'/kjdata/',name,'/$do_date', "'",' into table $db.',name, ' partition (dt=',"'",'$do_date',"'",');') from seven

select CONCAT('load data inpath ',"'",'/kjdata/',name,'/$do_date', "'",' into table $db.',name, ' partition (dt=',"'",'$do_date',"'",');') from less
```

###### 13、haoop ha 状态查询

```
hdfs haadmin -getServiceState nn1
yarn rmadmin -getServiceState rm1
```

###### 14、sqlserver数据最后修改时间

```
SELECT name,modify_date FROM sys.all_objects WHERE  TYPE='u'  and  CONVERT(VARCHAR(30),modify_date,23) >='2021-01-01' ORDER BY modify_date DESC;
```

###### 15、远程连接命令

```
ssh root@ip
```

###### 16、hdfs指定目录下进行排序

```
hadoop fs -du -s -h  /kjdata/* |sed 's/ //'| sort -rn 目录内文件的大小

hadoop fs -du -s -h  /kjdata |sed 's/ //'| sort -rn 目录大小
```

###### 17、shell 指定某个日期的5天前日期

```
date -d "-5 day 2021-11-17" +%Y-%m-%d
```

###### 18、准备professional

```
 基本的技能，不能有问题，必须熟读于心，出口即来。
基本的基本技能，
Java-se,
mysql,
mysql的优化，
MySQL的底层引擎，
maven,
linux,
shell,git ,idea,githup,
spring,springboot,mybatis,mybatisplus,sprintmvc,
hadoop,
hive,
zookeeper,
kafka,
sqoop,
datax,
canal,
maxwell,
hbase,
数仓，
建模，
sql分析，
数仓总结，
scala,
sparkcore,
spark rdd,
sparkstreaming,
sparksql,
spark实时项目，
sparkstreaming,
sparksql离线项目，
flink,flink的基本知识。核心概念，运行架构，flink的实时项目，背诵宝典
，k8s,docker,cdh,hdp,hadoop源码，spark源码，hive源码，修改flume源码，阅读源码和修改源码的经历，后出的一些比较活的概念，flink的实时数仓，传统的数仓的不断升级，doirs,clickhouse,datalake,hudi,phoenix
基本框架的的核心概念和原理，在使用的过程中遇到的问题，自己是如何解决的
一定要突出数据量，解决了达到的数据量，数据量越大，遇到的问题越复杂，其中的坑越多，才能有更好的解决思路
结合业务场景提供解决问题的思路。
```

###### 19、idea 常用键

```
Ctrl + H 查看父类和子类的关系
Ctrl + N 查找class
Ctrl + F 在当前文件中查找
Ctrl + R 当前文件替换
Ctrl + shift F 项目中查找
shift + shift 快速搜索
Ctrl + shift N 查找文件
Ctrl + shift R 在项目中替换
Ctrl + e 打开最近的文件
Ctrl + Tab 切换文件
Alt + 上下方向键 : 快速切换方法
```

###### 20、shell调用mysql的存储过程部署用于dolphinscheduler

```
#!/bin/bash

HOSTNAME="10.10.80.27" #数据库信息

PORT="3206" #端口

USERNAME="root" #用户	

PASSWORD="decent2020" #密码

DBNAME="interest" #数据库

TABLENAME="t_eos_szlxjsrqh" #表

mysql="/data/program/mysql-8.0.22/bin/mysql" #mysql 客户端

echo "Begin execute gn_lxjz_rjdy_SZ" #打印输出

dbt=`$mysql -h${HOSTNAME} -P${PORT} -u${USER} -p${PASSWORD} -D${DBNAME} -e "SELECT DATE_FORMAT(jsrq,'%Y-%m-%d') jsrq  FROM ${TABLENAME}"|awk NR==2` #参数获取

execSql="call gn_lxjz_rjdy_SZ(DATE_FORMAT('$dbt','%Y-%m-%d'))" #参数格式化

echo $execSql

$mysql -h${HOSTNAME} -P${PORT} -u${USER} -p${PASSWORD} -D${DBNAME} -e "${execSql}" #执行存储过程
```

